{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Environment Setup\n",
    "\n",
    "# In this example, we use Conda for managing the environment and dependencies.\n",
    "# Based on a few experiments, Conda seems to be the optimal choice for this project due to its comprehensive \n",
    "# package management and environment isolation capabilities. It allows us to manage dependencies and avoid \n",
    "# conflicts efficiently. However, you are not restricted to using Conda. Feel free to use any environment \n",
    "# management tool that you are comfortable with, such as virtualenv, pipenv, or others.\n",
    "\n",
    "# Note that some dependencies and library versions might change and get updated in the future. \n",
    "# From the date of writing this code to the time you might be running it, certain packages may have newer \n",
    "# versions or changes in compatibility. Always ensure to check for the latest versions and compatibility \n",
    "# issues. This can be done by reviewing the package documentation or using version constraints in your \n",
    "# package installation commands.\n",
    "\n",
    "# Performance may vary depending on internet speed, system specifications, and other factors.\n",
    "# Cloud provisioning and model loading times can differ across systems. High-speed internet can significantly\n",
    "# reduce the time for downloading and installing packages, while a system with higher specifications (such as\n",
    "# more CPU cores, RAM, and faster storage) can speed up the environment setup and model training processes.\n",
    "# Ensure your system meets the recommended requirements for optimal performance. Additionally, cloud \n",
    "# provisioning can introduce variability in performance due to differences in network latency, server load, \n",
    "# and resource availability at different times.\n",
    "\n",
    "# These shell commands will download and install Miniconda, and set up the environment.\n",
    "\n",
    "# Create a directory for Miniconda\n",
    "!mkdir -p ~/miniconda3\n",
    "\n",
    "# Download the Intel-compatible Miniconda installer\n",
    "!curl https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-x86_64.sh -o ~/miniconda3/miniconda.sh\n",
    "\n",
    "# Install Miniconda quietly\n",
    "!bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3\n",
    "\n",
    "# Clean up the installer\n",
    "!rm -rf ~/miniconda3/miniconda.sh\n",
    "\n",
    "# Initialize Miniconda for bash and zsh shells\n",
    "!~/miniconda3/bin/conda init bash\n",
    "!~/miniconda3/bin/conda init zsh\n",
    "\n",
    "# After running this cell, restart the kernel to apply changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Set Up Conda Environment\n",
    "\n",
    "# These commands set up a Conda environment specifically for this project.\n",
    "# Using a separate environment helps manage dependencies and avoid conflicts with other projects.\n",
    "\n",
    "# Create a new Conda environment named 'qai_hub' with Python 3.8\n",
    "!~/miniconda3/bin/conda create -n qai_hub python=3.8 -y\n",
    "\n",
    "# Activate the new environment\n",
    "# Note: The following command won't actually change the environment in the notebook.\n",
    "# You'll need to manually activate the environment in your terminal or JupyterLab.\n",
    "!source ~/miniconda3/bin/activate qai_hub\n",
    "\n",
    "# Install the Qualcomm AI Hub Python client and other necessary libraries\n",
    "# This step might take a few minutes depending on your internet speed and system performance\n",
    "!~/miniconda3/bin/conda run -n qai_hub pip install qai-hub onnx onnxruntime transformers\n",
    "\n",
    "# Install the development version of transformers\n",
    "# We use the development version to ensure compatibility with the Phi-3 model.\n",
    "!~/miniconda3/bin/conda run -n qai_hub pip uninstall -y transformers && pip install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Configure Qualcomm AI Hub\n",
    "\n",
    "import qai_hub as hub\n",
    "\n",
    "# Configure the Qualcomm AI Hub with your API token.\n",
    "# This allows us to interact with Qualcomm's cloud services for model deployment and profiling.\n",
    "# Replace \"INSERT_YOUR_API_TOKEN_HERE\" with your actual Qualcomm API token.\n",
    "hub.configure(api_token=\"INSERT_YOUR_API_TOKEN_HERE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Load the Model\n",
    "\n",
    "# We use the Microsoft Phi-3-mini-128k-instruct-onnx model for this experiment.\n",
    "# This model is chosen for its balance between performance and resource requirements.\n",
    "# Ensure compatibility with the development version of transformers.\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load model and tokenizer with trust_remote_code=True\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct-onnx\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct-onnx\", trust_remote_code=True)\n",
    "\n",
    "# Example usage: Generate text based on a user prompt\n",
    "messages = [{\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"}]\n",
    "inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "\n",
    "# Generate text\n",
    "outputs = model.generate(inputs, max_new_tokens=32)\n",
    "text = tokenizer.batch_decode(outputs)[0]\n",
    "print(text)\n",
    "\n",
    "# Use a pipeline as a high-level helper for text generation tasks\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"microsoft/Phi-3-mini-128k-instruct-onnx\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 (Optional): Convert Model to ONNX (If using a different model)\n",
    "\n",
    "# If you are using a different model that is not already in ONNX format, you need to convert it to ONNX.\n",
    "# Below is an example of converting a PyTorch model to ONNX format.\n",
    "\n",
    "import torch\n",
    "import torch.onnx\n",
    "from transformers import AutoModel\n",
    "\n",
    "# Replace 'your_model_here' with the model you are using\n",
    "model_name = 'your_model_here'\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Example input tensor, adjust the shape as per your model's requirement\n",
    "example_input = torch.rand(1, 3, 224, 224)\n",
    "\n",
    "# Export the model to ONNX format\n",
    "torch.onnx.export(model, example_input, \"your_model.onnx\", export_params=True, opset_version=11)\n",
    "\n",
    "print(\"Model converted to ONNX format successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Initial Quantization\n",
    "\n",
    "# Quantization is the process of reducing the precision of the numbers used to represent a model's parameters.\n",
    "# We use Dynamic Post-Training Quantization (PTQ) to reduce the model size and improve inference speed \n",
    "# without needing to retrain the model. Quantization is crucial for deploying models on edge devices \n",
    "# with limited computational resources and power budgets, like smartphones and IoT devices.\n",
    "\n",
    "# Dynamic PTQ is chosen because it quantizes the model weights and dynamically quantizes the activations \n",
    "# during inference. This method strikes a balance between model accuracy and performance improvements, \n",
    "# making it suitable for on-device deployment where computational resources are constrained.\n",
    "\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "# Path to the ONNX model\n",
    "model_path = \"path_to_your_model/phi-3-mini-128k-instruct-onnx\"\n",
    "quantized_model_path = \"path_to_your_model/phi-3-mini-128k-instruct-onnx-quantized.onnx\"\n",
    "\n",
    "# Quantize the model\n",
    "# QuantType.QInt8 reduces the precision of weights from 32-bit floating-point to 8-bit integer, \n",
    "# reducing the model size by approximately 75%. For example, a 100MB model would be reduced to around 25MB.\n",
    "quantize_dynamic(model_path, quantized_model_path, weight_type=QuantType.QInt8)\n",
    "\n",
    "print(\"Model quantized successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Neural Network Graph Capture\n",
    "\n",
    "# Capturing the neural network graph is essential for understanding the structure and behavior of the model.\n",
    "# This step helps in visualizing the model architecture and identifying potential optimization opportunities.\n",
    "# The captured graph can be used for debugging, optimizing, and ensuring the correctness of the model.\n",
    "\n",
    "import torch\n",
    "from torch.onnx import export\n",
    "import onnx\n",
    "\n",
    "# Load the quantized ONNX model\n",
    "onnx_model = onnx.load(quantized_model_path)\n",
    "\n",
    "# Visualize the graph (this example uses Netron, which should be installed separately)\n",
    "import netron\n",
    "netron.start(quantized_model_path)\n",
    "\n",
    "print(\"Neural network graph captured and visualized successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: On-Device Compilation\n",
    "\n",
    "# On-device compilation involves compiling the model specifically for the target hardware.\n",
    "# This step ensures that the model is optimized for the device's architecture, making use of hardware \n",
    "# acceleration features available on the target device, such as the Neural Processing Unit (NPU).\n",
    "\n",
    "# Compile the traced model for the target device\n",
    "compile_job = hub.submit_compile_job(\n",
    "    model=quantized_model_path,\n",
    "    device=hub.Device(\"Samsung Galaxy S24 Ultra\"),\n",
    "    input_specs=dict(image=(1, 3, 224, 224)),\n",
    "    options=\"--target_runtime qnn_context_binary\"\n",
    ")\n",
    "\n",
    "# Check the status of the compile job\n",
    "compile_status = compile_job.get_status()\n",
    "print(f\"Compile job status: {compile_status}\")\n",
    "\n",
    "# Get the compiled model\n",
    "compiled_model = compile_job.get_target_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Hardware Acceleration\n",
    "\n",
    "# Utilizing hardware acceleration involves leveraging the specific capabilities of the target device's \n",
    "# hardware, such as the NPU, GPU, or specialized accelerators. This step ensures that the model runs \n",
    "# efficiently, taking advantage of the device's full potential.\n",
    "\n",
    "# For instance, NPUs can execute certain operations 10-100 times faster than CPUs due to their parallel \n",
    "# processing capabilities, which can significantly speed up inference times.\n",
    "\n",
    "# Submit a profile job to run the compiled model on the device with hardware acceleration\n",
    "profile_job = hub.submit_profile_job(model=compiled_model, device=hub.Device(\"Samsung Galaxy S24 Ultra\"))\n",
    "\n",
    "# Check the status of the profile job\n",
    "profile_status = profile_job.get_status()\n",
    "print(f\"Profile job status: {profile_status}\")\n",
    "\n",
    "# Download profile results\n",
    "profile_results = profile_job.download_profile()\n",
    "print(f\"Profile results: {profile_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Second Round of Quantization\n",
    "\n",
    "# Perform a second round of quantization directly on the device using Qualcomm AI Hub.\n",
    "# This step leverages the specific hardware capabilities of the target device to further optimize the model.\n",
    "# On-device quantization can take advantage of the exact hardware characteristics, such as the specific \n",
    "# capabilities of the Neural Processing Unit (NPU), to achieve better performance and efficiency.\n",
    "\n",
    "second_quantization_job = hub.submit_quantize_job(model=compiled_model, device=hub.Device(\"Samsung Galaxy S24 Ultra\"))\n",
    "\n",
    "# Check the status of the quantization job\n",
    "second_quantization_status = second_quantization_job.get_status()\n",
    "print(f\"Second quantization job status: {second_quantization_status}\")\n",
    "\n",
    "# Get the further quantized model\n",
    "second_quantized_model = second_quantization_job.get_target_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11: Evaluate and Compare Performance\n",
    "\n",
    "# Evaluate and compare the performance of both quantized models by submitting inference jobs.\n",
    "# This step involves running the models with sample inputs and measuring various performance metrics \n",
    "# such as inference time, memory usage, and power consumption.\n",
    "\n",
    "# Submit inference jobs for both models\n",
    "inference_job_initial = hub.submit_inference_job(model=compiled_model, device=hub.Device(\"Samsung Galaxy S24 Ultra\"), inputs={\"input_name\": example_input})\n",
    "inference_job_second = hub.submit_inference_job(model=second_quantized_model, device=hub.Device(\"Samsung Galaxy S24 Ultra\"), inputs={\"input_name\": example_input})\n",
    "\n",
    "# Get inference results\n",
    "inference_results_initial = inference_job_initial.download_output_data()\n",
    "inference_results_second = inference_job_second.download_output_data()\n",
    "\n",
    "# Evaluate and compare performance metrics\n",
    "performance_initial = profile_job.download_profile()\n",
    "performance_second = second_quantization_job.download_profile()\n",
    "\n",
    "print(f\"Initial Quantized Model Performance: {performance_initial}\")\n",
    "print(f\"Second Quantized Model Performance: {performance_second}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
